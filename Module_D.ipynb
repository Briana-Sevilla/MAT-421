{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNARJB8A4JAvaUvWfAf4ign",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Briana-Sevilla/MAT-421/blob/main/Module_D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Homework Set 5: Module D**\n"
      ],
      "metadata": {
        "id": "n_TMShafN7Fo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 1.1: Introduction of Linear Algebra"
      ],
      "metadata": {
        "id": "WUL5L-ASOOzp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear algebra is a branch of mathematics essential to multiple areas of study like data science, physics, machine learning, and mathematics. It includes concpets like vectors, eigenvalues, matrices, orthogonality, transformations, etc."
      ],
      "metadata": {
        "id": "PnbpRfzFQjs7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\\\\\n",
        "\n",
        "---\n",
        "\n",
        "\\\\\n",
        "## Section 1.2: Elements of Linear Algebra"
      ],
      "metadata": {
        "id": "L1n7hawsOniQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The linear algebra concepts we will review will be confined by euclidean space. This space is in 2-dimensions or 3-dimensions and is ideal to imitate the physical world and how vectors can span on them. Since we use coordinates to designate the location of a point, we are limited to the use of the following kinds of vectors:\n",
        "\n",
        "- $\\ v ‚Éó \\in ‚Ñù^2 = (x,y) $\n",
        "- $\\ v ‚Éó \\in ‚Ñù^3 = (x,y,z) $\n"
      ],
      "metadata": {
        "id": "0dgX0brn0DFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2.1: Linear Spaces\n",
        "#### 1.2.1.1: Linear Combinations"
      ],
      "metadata": {
        "id": "el_mmHQHytsD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Say you have a subset of vectors $S$, such that $\\ S = \\{s‚Éó_1,s‚Éó_2,...,s‚Éó_n\\}$. You can make a new vector, called a linear combination, when you multiply the vectors in $S$ by a scalar and add them all together. This idea is represented by:\n",
        "\n",
        "$\\ \\sum_{i=1}^n \\alpha_i s ‚Éó _i$,\n",
        "\n",
        "where\n",
        "- $\\alpha$ = scalar\n",
        "- $\\ s‚Éó_i $ are the vectors in set $S$\n",
        "\n",
        "For example, let's say we have a set $V$ that has\n",
        "$v‚Éó_1 = \\begin{bmatrix}\n",
        "  1 \\\\\n",
        "  2 \\\\\n",
        " \\end{bmatrix}$\n",
        " and\n",
        "$ v‚Éó_2 = \\begin{bmatrix}\n",
        " 3 \\\\\n",
        " 4 \\\\\n",
        " \\end{bmatrix}$ as its elements.\n",
        "\n",
        " By multiplying each vector by the scalar $\\ 2$, we get\n",
        "\n",
        "$2 \\cdot v‚Éó_1 =\n",
        "\\begin{bmatrix}\n",
        "2 \\cdot 1 \\\\\n",
        "2 \\cdot 2 \\\\\n",
        "\\end{bmatrix} =\n",
        "\\begin{bmatrix}\n",
        "2 \\\\\n",
        "4 \\\\\n",
        "\\end{bmatrix}$\n",
        "\n",
        "and\n",
        "\n",
        "$2 \\cdot v‚Éó_2 =\n",
        "\\begin{bmatrix}\n",
        "2 \\cdot 3 \\\\\n",
        "2 \\cdot 4 \\\\\n",
        "\\end{bmatrix} =\n",
        "\\begin{bmatrix}\n",
        "6 \\\\\n",
        "8 \\\\\n",
        "\\end{bmatrix}$\n",
        "\n",
        "Now lets add these!\n",
        "\n",
        "$\\begin{bmatrix}\n",
        "2 \\\\\n",
        "4 \\\\\n",
        "\\end{bmatrix} +\n",
        "\\begin{bmatrix}\n",
        "  6 \\\\\n",
        "  8 \\\\\n",
        " \\end{bmatrix} =\n",
        "\\begin{bmatrix}\n",
        "8 \\\\\n",
        "12 \\\\\n",
        "\\end{bmatrix}$\n",
        "\n",
        "This new vector is a linear combination of $\\ v‚Éó_1$ and $\\ v‚Éó_2$.\n",
        "\n",
        "\n",
        "These linear combinations create a **linear subpace**, which is a **span** of a set of vectors. A **span** of a set of vectors is the set of all linear combinations of the elements inside the set.\n",
        ">Note: a linear subspace of $B$ is a subset $V ‚äÜ B$. In addition, it is also closed under vector addition and scalar multiplication, which means that if $\\ v‚Éó_1,v‚Éó_2 \\in V$, and $\\ \\alpha \\in ‚Ñù $, then\n",
        "$\\ v‚Éó_1+v‚Éó_2 \\in V$ and $\\ \\alpha v‚Éó_1 \\in V$.\n",
        "\n",
        ">Note: 0 is in every linear subspace\n",
        "\n",
        ">Note: The column space of a matrix is the span of its columns\n",
        "\n",
        ">Note: Every span is a linear subspace\n"
      ],
      "metadata": {
        "id": "J1IcCMmC8sy4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.2.1.2: Linear Dependence and Dimension\n"
      ],
      "metadata": {
        "id": "c5SLwjVKQugS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Linear Independence** can be seen when a list of vectors cannot be used to make a linear combination of the others. A list of vectors is linearly independent if the have the following property:\n",
        "\n",
        "$\\ \\forall i, v‚Éó_i \\notin span(\\{v‚Éó_j:j \\neq i\\})$\n",
        "\n",
        "For example, say we have the following three vectors:\n",
        "$\\ u‚Éó_1=\n",
        "\\begin{bmatrix}\n",
        "2 \\\\\n",
        "2 \\\\\n",
        "0 \\\\\n",
        "\\end{bmatrix}$ ,\n",
        "$\\ u‚Éó_2=\n",
        "\\begin{bmatrix}\n",
        "2 \\\\\n",
        "0 \\\\\n",
        "1 \\\\\n",
        "\\end{bmatrix}$ , and\n",
        "$\\ u‚Éó_3\n",
        "\\begin{bmatrix}\n",
        "0 \\\\\n",
        "1 \\\\\n",
        "2 \\\\\n",
        "\\end{bmatrix}$.\n",
        "\n",
        "$\\ u‚Éó_1$ is linearly independent from $\\ u‚Éó_2$ and $\\ u‚Éó_3$ since they cannot be used to make a linear combination that equals $\\ u‚Éó_1$. The same is true for the other vectors. No two vectors can form a linear combination equal to the last one. Thus, this list of vectors are linearly independent.\n",
        "\n",
        "**Linearly Dependent**: a list of vectors that are not linearly independent.\n",
        "\n",
        "A **basis** is a set of linearly independent vectors that generate all elements of a vector space (they span that vector space).\n",
        "\n",
        ">Note: Vector spaces can have more than one basis; each one must have the same number of elements (this is its dimension)\n",
        "\n",
        "\n",
        "For example, a matrix's column rank is the dimension of its column space.  "
      ],
      "metadata": {
        "id": "iv0OP6uTjS_F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2.2: Orthogonality\n",
        "#### 1.2.2.1: Orthogonal Bases"
      ],
      "metadata": {
        "id": "jzBS7gj4tIHH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Norm** $\\ = ||u‚Éó|| = \\sqrt{\\sum_1^nu_i^2}$\n",
        "- Magnitude of a vector\n",
        "- Non-negative number\n",
        "\n",
        "**Inner Product (aka dot product)**: $\\ <u‚Éó,v‚Éó> = u‚Éó \\cdot v‚Éó = \\sum_i^nu_iv_i$\n",
        "- Helps define angle between two vectors\n",
        "- Example: $\\ <1,2,3> \\cdot <4,5,6> = 1\\cdot4+2\\cdot5+3\\cdot6 = 4+10+18 = 32$\n",
        "\n",
        "**Orthogonal Vectors**: a list of vectors are orthogonal if their dot product is 0.\n",
        "- This means the vectors are perpendicular\n",
        "\n",
        "**Orthonormal Vectors**: a list of vectors are orthonormal if they are orthogonal and if the norm of each of them is equal to 1."
      ],
      "metadata": {
        "id": "5-RvPrR1tUhn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.2.2.2: Best Approximation Theorem\n",
        "\n"
      ],
      "metadata": {
        "id": "W5BdvJ2wzRN6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Orthogonal Projection** $\\ = ùí´_{ùí∞}v‚Éó= \\sum_{j-1}^m <v‚Éó,q‚Éó_j>q‚Éó_j$\n",
        "\n",
        "where:\n",
        "- $\\ ùí∞ \\subseteq ùí´$ is a linear subspace\n",
        "- $\\ q‚Éó _1,...,q‚Éó_m$ are orthonormal basis\n",
        "- $\\ v‚Éó$ is a vector such that $\\ v‚Éó \\in V$\n",
        "\n",
        "**Best Approximation Theorem** $= ||v‚Éó-ùí´_ùí∞v‚Éó|| ‚â§ ||v‚Éó-u‚Éó||$, where the terms are the same as the one above.\n",
        "\n",
        "**Orthogonal Decomposition**: If $W$ is a subspace of $U$, there exists $\\ w‚Éó\\in W$ and $\\ w‚Éó^‚ä• \\in W $, for any vector $ u‚Éó \\in U$ such that\n",
        "\n",
        "$u‚Éó = w‚Éó + w‚Éó^‚ä•$,\n",
        "\n",
        "where\n",
        "- $w‚Éó$ is the projection of $v‚Éó$ onto $W$\n",
        "- $w‚Éó^‚ä•$ is the part of $v‚Éó$ orthogonal to $W$\n",
        "\n",
        "Orthogonal matrices have the following property:\n",
        "\n",
        "$ùí´ = QQ^T = I$\n",
        "\n",
        "where\n",
        "- $Q$ is an orthogonal matrix\n",
        "- $Q^T$ is the transpose of $Q$\n",
        "- $I$ is the identity matrix\n",
        ">Note: $Q^{-1} = Q^T$"
      ],
      "metadata": {
        "id": "yQSDeYHE03vc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2.3: Gram-Schmidt Process"
      ],
      "metadata": {
        "id": "e7BJa5us2VpS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **Gram-Schmidt Theorem** says that there exists an orthonormal basis $\\ q‚Éó _1,...,q ‚Éó_m$ of $span(v ‚Éó_1,...,v ‚Éó_m)$ if $a ‚Éó_1,...a ‚Éó_m \\in ‚Ñù^n$ are linearly independent."
      ],
      "metadata": {
        "id": "0eZHfbox4_ZP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2.4: Eigenvalues and Eigenvectors"
      ],
      "metadata": {
        "id": "ChnwaHkK3ywZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">Note: The following assumed we are working in $‚Ñù^d$\n",
        "\n",
        "Let matrix $A \\in ‚Ñù^{d \\times d}$ (this means it's a square matrix with dimensions $d \\times d$). Then $ùúÜ \\in ‚Ñù$ is an eigenvalue of matrix $A$ if:\n",
        "\n",
        "$Ax‚Éó=ùúÜ x‚Éó$, where $x‚Éó$ is a nonzero eigenvector\n",
        "\n",
        ">Note: Not every matrix has an eigenvalue\n"
      ],
      "metadata": {
        "id": "Enm4duZO37Dx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.2.4.1: Diagonalization of Symmetric Matrices"
      ],
      "metadata": {
        "id": "YUJRYnT9SiA1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A diagonal matrix looks like the following:\n",
        "$\\begin{bmatrix}\n",
        "ùúÜ_1 & 0 & 0 \\\\\n",
        "0 & ùúÜ_2 & 0 \\\\\n",
        "0 & 0 & ùúÜ_3 \\\\\n",
        "\\end{bmatrix}$\n",
        "\n",
        "Matrix $A$ is similar to matrix $B$, denoted by $A ‚àº B$, if an invertible nonsingular matrix $P$ exists such that:\n",
        "\n",
        "$A = PBP^{-1}$\n",
        "\n",
        "or\n",
        "\n",
        "$AP = PB$\n",
        "\n",
        ">Note: The order of matrix multiplication matters! It is not always the case when $AB = BA$!\n",
        "\n",
        ">Note: A matrix is nonsingular id its determinant does NOT equal zero!\n",
        "\n",
        ">Note: the matrices are squares!\n",
        "\n",
        "So, if matrix $A \\sim D$, where $D$ is the diagonal matrix with distinct eigenvalues $ùúÜ_1,...ùúÜ_d$ as its entries, then\n",
        "\n",
        "$A = PDP^{-1}$\n",
        "\n",
        "$AP=PD$, if $P$ has $d$ columns\n",
        "\n",
        "which suggests that\n",
        "\n",
        "$Ap‚Éó_i = ùúÜ_ip‚Éó_i$\n",
        "\n",
        "Any two eigenvectors from distinct eigenvalues are orthogonal if matrix $A$ is symmetric.\n",
        "\n",
        "A matrix is orthogonally diagonalizable if an orthogonal matrix $P$ and a diagonal matrix $D$ such that:\n",
        "\n",
        "$A = PDP^T=PDP^{-1}$\n",
        "\n",
        ">Note: Orthogonally diagonalizable matrices are symmetric\n",
        "\n",
        "**Symmetric Matrices** have the following properties:\n",
        "- They are $n \\times n$ square matrices\n",
        "- The matrix is orthogonally diagonalizable\n",
        "- They have $n$ real eigenvalues (they can be repeated)\n",
        "- The multiplicity, $m$ of eigenvalues means that the eigenspace for them is $m$-dimensional\n",
        "- The eigenvectors that belong to distinct eigenvalues are orthogonal\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UxghONEySr87"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.2.4.2: Constrained Optimization"
      ],
      "metadata": {
        "id": "6PKFUuDi6PF3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following is a good way to optimize a problem:\n",
        "\n",
        "when $x‚Éó=v‚Éó_1$:\n",
        "\n",
        "$min_{x \\neq 0} \\frac{x‚Éó^T Ax‚Éó}{x‚Éó^Tx‚Éó}=ùúÜ_1$\n",
        "\n",
        "\\\\\n",
        "\n",
        "and\n",
        "\n",
        "\\\\\n",
        "\n",
        "when $x‚Éó=v‚Éó_n$:\n",
        "\n",
        "$max_{x \\neq 0} \\frac{x‚Éó^T Ax‚Éó}{x‚Éó^Tx‚Éó}=ùúÜ_n$\n",
        "\n",
        "where:\n",
        "- $A$ is a $n \\times n$ symmetric matrix\n",
        ">Note: This means it is orthogonally diagonalizable: $A=PDP^{-1}$. Also, the columns of $P$ are orthonormal eigenvectors of $A$ and $D$ is a diagonal matrix with $ùúÜ_1 ‚â§ ùúÜ_2 \\le ... \\le ùúÜ_n$ as its diagonal values."
      ],
      "metadata": {
        "id": "cmHoQ01U8JIT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\\\\\n",
        "\n",
        "---\n",
        "\n",
        "\\\\\n",
        "## Section 1.3: Linear Regression\n"
      ],
      "metadata": {
        "id": "E-luYUoyO4Io"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " #### 1.3.1: QR Decomposition"
      ],
      "metadata": {
        "id": "1GFJbWao_PQL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**QR Decomposition**: a procedure used to solve the linear least squares problem, ultimately helping us find the best-fit solution.\n",
        "\n",
        "To use this method, we need to find the orthonormal basis $span(a‚Éó_1,...,a‚Éó_m)$ from a linearly independent set of $span(a‚Éó_1,...,a‚Éó_m)$. Luckily, we already know a process that can give us what we want: the Gram-Schmidt Process.\n",
        "\n",
        "The Gram-Schmidt Process gives us the following output:\n",
        "\n",
        "$A = QR$ (this form is called a QR decomposition)\n",
        "\n",
        "where\n",
        "- R is a $m \\times m$ upper triangular matrix\n",
        "- Q is a $n \\times m$ orthogonal matrix\n",
        "- A is a $n \\times m$ matrix"
      ],
      "metadata": {
        "id": "X0BNNYlBA5Fq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3.2: Least-Squares Problems"
      ],
      "metadata": {
        "id": "uVaZMNU1E_KQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The least-squares problem can be represented by the following expression:\n",
        "\n",
        "$min_{x \\in ‚Ñù^m}||Ax‚Éó-b‚Éó||$\n",
        "\n",
        "where\n",
        "- $A \\in ‚Ñù^{n \\times m}$ is a $n \\times m$ matrix\n",
        "- $b‚Éó \\in ‚Ñù^n$ is a vector\n",
        "- $x‚Éó$ is the vector we solve for\n",
        "\n",
        "Steps:\n",
        "1) Find linear combination of the columns of $A$ such that minimizes the following:\n",
        "\n",
        "$\\sum_{i=1}^n(yÃÇ_i-b_i)^2$\n",
        "\n",
        "where $yÃÇ_i=\\sum_{j=1}^mx_ja_{i,j}$\n",
        "\n",
        "2) Use the orthogonal projection on the column space of $A$ such that\n",
        "\n",
        "$bÃÇ = ùí´_{col(a)}b‚Éó$\n",
        "\n",
        "3) Solve for vector $xÃÇ$ in\n",
        "$AxÃÇ=bÃÇ$. This will be our solution\n",
        "\n",
        "Another way to solve for the least-squares problem is to use the **normal equations**:\n",
        "\n",
        "$A^TAx‚Éó=A^Tb‚Éó$\n",
        "\n",
        "in the least-squares problem:\n",
        "\n",
        "$min_{x\\in ‚Ñù^m}||Ax‚Éó-b‚Éó||$.\n",
        "\n",
        "The solution of the normal equations is $(A^TA)^{-1}A^Tb‚Éó$ since $A^TA$ is invertible if $A$ has linearly independent columns\n",
        "\n",
        "Since this approach is not ideal due to the numerous numerical issues, it is best to use QR decomposition instead. This means that the solution to the least-squares problem is the following:\n",
        "\n",
        "$Rx^*=Q^Tb‚Éó$"
      ],
      "metadata": {
        "id": "jtd2ZOaYFFJP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3.3: Linear Regression"
      ],
      "metadata": {
        "id": "fYCjuo3LLuAa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When given a list of data points, our goal is to find a function that fits the data best. That is, finding the coefficients that minimize the following:\n",
        "\n",
        "$\\sum_{i=1}^n(y_i-yÃÇ_i)^2$,\n",
        "\n",
        "where\n",
        "\n",
        "- Predicted values $= yÃÇ_i = Œ≤_0+\\sum_{j=1}^dŒ≤_jx_{ij}$\n",
        "- Actual value $= y_i$\n",
        "- Coefficients $= Œ≤_j$"
      ],
      "metadata": {
        "id": "cl2GNmILLyPh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\\\\n",
        "\n",
        "**The End**"
      ],
      "metadata": {
        "id": "rFl68AOAP7Kq"
      }
    }
  ]
}